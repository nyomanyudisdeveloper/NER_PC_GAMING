{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraryu for load file pkl and txt\n",
    "import pickle  \n",
    "import json \n",
    "\n",
    "# import libnrary for proses data manipulation\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# import for process stemming using sastrawi \n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load model and tag,word dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_NER.pkl\",\"rb\") as file_1:\n",
    "    model = pickle.load(file_1)\n",
    "\n",
    "with open(\"wordDict.json\",\"r\") as file_2:\n",
    "    word2idx = json.load(file_2)\n",
    "\n",
    "with open(\"tagDict.json\",\"r\") as file_3:\n",
    "    tag2idx = json.load(file_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Inferential Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data dummy for model inferential \n",
    "data_inferential =  \"Fortnite Brp fps kk \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Replace enter with space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace newline into space\n",
    "data_inferential = data_inferential.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3.2 Remove unnecessary characters (Only alphabetic remaining)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forza horizon   bsa om  bsa skalian di install  '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = \"\"\n",
    "for char in data_inferential:\n",
    "    if (char == \" \" or char.isalpha()) and char != \"Â²\":\n",
    "        result+= char\n",
    "    else:\n",
    "        result += \" \"\n",
    "\n",
    "data_inferential = result\n",
    "data_inferential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3.3 Change letters to lowercase***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forza horizon   bsa om  bsa skalian di install  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data inferential to lower case \n",
    "data_inferential = data_inferential.lower()\n",
    "data_inferential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3.4 Remove White Spaces***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forza horizon   bsa om  bsa skalian di install'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove whitespaces in data inferential \n",
    "data_inferential = data_inferential.strip()\n",
    "data_inferential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3.5  Tokenization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forza',\n",
       " 'horizon',\n",
       " '',\n",
       " '',\n",
       " 'bsa',\n",
       " 'om',\n",
       " '',\n",
       " 'bsa',\n",
       " 'skalian',\n",
       " 'di',\n",
       " 'install']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inferential_token = data_inferential.split(\" \")\n",
    "data_inferential_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3.6 Change Slang Word into Normal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file csv for slang dictionary \n",
    "slang = pd.read_csv(\"../dataset/Slang2.csv\")\n",
    "slang_dict = dict(zip(slang['slang'], slang['formal']))\n",
    "\n",
    "# del specific key because it is not neccessary for PC GAMING NER scenario \n",
    "del slang_dict['main']\n",
    "del slang_dict['banget']\n",
    "del slang_dict['uhh']\n",
    "del slang_dict['takut']\n",
    "del slang_dict['da']\n",
    "del slang_dict['uhhh']\n",
    "\n",
    "# edit specific key in slang_dict \n",
    "slang_dict['dahhhh'] = 'sudah'\n",
    "slang_dict['kalo'] = 'kalau'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forza',\n",
       " 'horizon',\n",
       " '',\n",
       " '',\n",
       " 'bisa',\n",
       " 'om',\n",
       " '',\n",
       " 'bisa',\n",
       " ' sekalian',\n",
       " 'di',\n",
       " 'install']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_result = []\n",
    "for word in data_inferential_token:\n",
    "    steming_slang = slang_dict.get(word)\n",
    "    if steming_slang == None:\n",
    "        list_result.append(word)\n",
    "    else:\n",
    "        try:\n",
    "            if(math.isnan(steming_slang)):\n",
    "                list_result.append(word)\n",
    "            else:\n",
    "                list_result.append(steming_slang)\n",
    "        except:\n",
    "            list_result.append(steming_slang)\n",
    "\n",
    "data_inferential_token = list_result\n",
    "data_inferential_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3.7 Stemming***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object that use to stemming word in indonesia using Sastrawati\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_skip_steming_word = ['kinemaster','setingan','bekasi','seandainya','seting','rohan','lemot','kesing','diseting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forza',\n",
       " 'horizon',\n",
       " '',\n",
       " '',\n",
       " 'bisa',\n",
       " 'om',\n",
       " '',\n",
       " 'bisa',\n",
       " 'sekali',\n",
       " 'di',\n",
       " 'install']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_result = []\n",
    "for word in data_inferential_token:\n",
    "    stemmed_word = word\n",
    "    if word not in list_skip_steming_word:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "    list_result.append(stemmed_word)\n",
    "data_inferential_token = list_result\n",
    "data_inferential_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forza',\n",
       " 'horizon',\n",
       " '',\n",
       " '',\n",
       " 'bisa',\n",
       " 'om',\n",
       " '',\n",
       " 'bisa',\n",
       " 'sekali',\n",
       " 'di',\n",
       " 'install']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Steming using manual word\n",
    "list_kata_dasar = ['setting','packing','offline','pc','seting','memory','software','ssd','halo','render','ongkir','ganti','upgrade','vga','mobo','case','casing','install','keyboard','ddr','processor','hdd','storage']\n",
    "list_result = []\n",
    "for token in data_inferential_token:\n",
    "    word_result = token\n",
    "    for kata_dasar in list_kata_dasar:\n",
    "        if token.find(kata_dasar) != -1:\n",
    "            word_result = kata_dasar\n",
    "            break\n",
    "    list_result.append(word_result)\n",
    "\n",
    "data_inferential_token = list_result \n",
    "data_inferential_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3.8 Stopword***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate object that use to remove stopword using sastrawi\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_remover = stopword_factory.create_stop_word_remover()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    text = ' '.join(tokens)  # Convert list of tokens to a single string\n",
    "    filtered_text = stopword_remover.remove(text)  # Remove stopwords\n",
    "    return filtered_text.split()  # Convert the filtered string back to a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forza', 'horizon', 'om', 'sekali', 'install']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inferential_token = remove_stopwords(data_inferential_token)\n",
    "data_inferential_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 convert word into categorial nominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_word_to_nominal_category(list_word):\n",
    "    '''\n",
    "    This function is used to change list word to categorial nominal based on word_dict that created when \n",
    "    process modelling \n",
    "\n",
    "    parameter_description \n",
    "    ========================\n",
    "    list_word = list of word\n",
    "\n",
    "    example usage \n",
    "    ====================\n",
    "    data_inferential_token = ['untuk', 'recordd', 'mlbb', 'dann', 'pubg', 'kuat', 'kah', 'gan']\n",
    "    data_inferential_token  = convert_word_to_nominal_category(data_inferential_token)ata_inferential_token= remove_stopword(data_inferential_token)\n",
    "    data_inferential_token\n",
    "    '''\n",
    "    list_result = []\n",
    "    for word in list_word:\n",
    "        if word in word2idx:\n",
    "            index = word2idx[word]\n",
    "            list_result.append(index)\n",
    "        else:\n",
    "            list_result.append(0)\n",
    "    return list_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forza', 'horizon', 'om', 'sekali', 'install']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inferential_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[223, 145, 122]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inferential_token_id = convert_word_to_nominal_category(data_inferential_token)\n",
    "list_result = []\n",
    "for index in data_inferential_token_id:\n",
    "    if index != 0:\n",
    "        list_result.append(index)\n",
    "data_inferential_token_id = list_result\n",
    "total_word = len(data_inferential_token_id)\n",
    "data_inferential_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[223, 145, 122]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inferential_token_id = np.array([data_inferential_token_id])\n",
    "data_inferential_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[223, 145, 122,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inferential_token_id = pad_sequences(maxlen=40, sequences=data_inferential_token_id, padding=\"post\")\n",
    "data_inferential_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6, 58, 22]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(data_inferential_token_id)\n",
    "pred = np.argmax(pred, axis=-1)\n",
    "pred = pred[0][0:total_word]\n",
    "pred = np.array([pred])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded words: [['forza' 'horizon' 'install' 'Unknown' 'Unknown' 'Unknown' 'Unknown'\n",
      "  'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown'\n",
      "  'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown'\n",
      "  'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown'\n",
      "  'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown'\n",
      "  'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown']]\n",
      "Decoded tags: [['B-Game' 'ENPAD' 'B-Request']]\n"
     ]
    }
   ],
   "source": [
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "# Decode data_inferential_token_id\n",
    "decoded_words = np.vectorize(lambda x: idx2word.get(x, 'Unknown'))(data_inferential_token_id)\n",
    "# Decode y_pred\n",
    "decoded_tags = np.vectorize(lambda x: idx2tag.get(x, 'None'))(pred)\n",
    "\n",
    "# Check for 'None' values in decoded_tags and handle them\n",
    "for row in decoded_tags:\n",
    "    for i, tag in enumerate(row):\n",
    "        if tag == 'None':\n",
    "            # Handle the 'None' value appropriately, e.g., replace with a default tag or remove\n",
    "            row[i] = 'ENPAD'  # Replace 'None' with 'Unknown'\n",
    "\n",
    "print(\"Decoded words:\", decoded_words)\n",
    "print(\"Decoded tags:\", decoded_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           \tPred\n",
      "------------------------------\n",
      "forza          \tB-Game\n",
      "horizon        \tENPAD\n",
      "install        \tB-Request\n"
     ]
    }
   ],
   "source": [
    "print(\"{:15}\\t{}\".format(\"Word\", \"Pred\"))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for word, tag in zip(decoded_words[0], decoded_tags[0]):\n",
    "    print(\"{:15}\\t{}\".format(word, tag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
