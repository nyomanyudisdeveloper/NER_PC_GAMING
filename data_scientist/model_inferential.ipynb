{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraryu for load file pkl and txt\n",
    "import pickle  \n",
    "import json \n",
    "\n",
    "# import libnrary for proses data manipulation\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from file pkl\n",
    "with open('model.pkl', 'rb') as file_1:\n",
    "  model = pickle.load(file_1)\n",
    "\n",
    "with open('word_dict.txt', 'r') as file_1:\n",
    "  word_dict = json.load(file_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Pre Processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data dummy for model inferential \n",
    "data_inferential = \"met untuk bisa recordd MLBB kepulauan dann PUBG kuat kah gan?? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'met untuk bisa recordd MLBB kepulauan dann PUBG kuat kah gan '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def clean_special_character(text):\n",
    "    '''\n",
    "    This function is used to transofrm text so there are no special character ( alphabetic and numeric only)\n",
    "\n",
    "    parameter description\n",
    "    ===========================\n",
    "    text = question or regular sentence \n",
    "\n",
    "    usage example \n",
    "    ===================\n",
    "    data_inferential = \"untuk record MLBB dan PUBG kuat kah gan??\"\n",
    "    data_inferential = clean_special_character(data_inferential)\n",
    "    '''\n",
    "    result = \"\";\n",
    "    for char in text:\n",
    "        if char == \" \" or char.isalnum():\n",
    "            result+= char\n",
    "    return result\n",
    "\n",
    "# Remove special character in data inferential\n",
    "data_inferential = clean_special_character(data_inferential)\n",
    "data_inferential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'met untuk bisa recordd mlbb kepulauan dann pubg kuat kah gan '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data inferential to lower case \n",
    "data_inferential = data_inferential.lower()\n",
    "data_inferential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'met untuk bisa recordd mlbb kepulauan dann pubg kuat kah gan'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove whitespaces in data inferential \n",
    "data_inferential = data_inferential.strip()\n",
    "data_inferential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['met',\n",
       " 'untuk',\n",
       " 'bisa',\n",
       " 'recordd',\n",
       " 'mlbb',\n",
       " 'kepulauan',\n",
       " 'dann',\n",
       " 'pubg',\n",
       " 'kuat',\n",
       " 'kah',\n",
       " 'gan']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inferential_token = data_inferential.split(\" \")\n",
    "data_inferential_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['selamat',\n",
       " 'untuk',\n",
       " 'bisa',\n",
       " 'record',\n",
       " 'mlb',\n",
       " 'kepulauan',\n",
       " 'dan',\n",
       " 'pubg',\n",
       " 'kuat',\n",
       " 'kah',\n",
       " 'gan']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import file Slang2.csv\n",
    "slang = pd.read_csv(\"../dataset/Slang2.csv\")\n",
    "\n",
    "# Create a dictionary from the slang DataFrame for faster lookups\n",
    "slang_dict = dict(zip(slang['slang'], slang['formal']))\n",
    "\n",
    "def replace_slang(tokens):\n",
    "    '''\n",
    "    This function is used for replace slang word indonesia into standard word \n",
    "    example \"met\" become \"selamat\"\n",
    "\n",
    "    parameter description\n",
    "    =============================\n",
    "    tokens = list of word \n",
    "\n",
    "    example usage \n",
    "    ==============================\n",
    "    data_inferential_token = ['untuk', 'recordd', 'mlbb', 'dann', 'pubg', 'kuat', 'kah', 'gan']\n",
    "    data_inferential_token  = replace_slang(data_inferential_token)\n",
    "    '''\n",
    "    # Replace each token if it matches a slang term\n",
    "    list_result = [];\n",
    "    for token in tokens:\n",
    "        steming_slang = slang_dict.get(token)\n",
    "        if steming_slang == None:\n",
    "            list_result.append(token)\n",
    "        else:\n",
    "            try:\n",
    "                if(math.isnan(steming_slang)):\n",
    "                    list_result.append(token)\n",
    "                else:\n",
    "                    list_result.append(steming_slang)\n",
    "            except:\n",
    "                list_result.append(steming_slang)\n",
    "    return list_result\n",
    "    # return [token if math.isnan(slang_dict.get(token)) else slang_dict.get(token)  for token in tokens]\n",
    "\n",
    "# Apply the slang replacement function to the data_inferential_token\n",
    "data_inferential_token  = replace_slang(data_inferential_token)\n",
    "data_inferential_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['selamat',\n",
       " 'untuk',\n",
       " 'bisa',\n",
       " 'record',\n",
       " 'mlb',\n",
       " 'pulau',\n",
       " 'dan',\n",
       " 'pubg',\n",
       " 'kuat',\n",
       " 'kah',\n",
       " 'gan']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate object that can be used for Stemming word in indonesia\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def steming_word_sastrawi(list_word):\n",
    "    '''\n",
    "    This function is used for stemming word in indonesia \n",
    "    example word \"kepulauan\" become \"pulau\"\n",
    "\n",
    "    parameter description \n",
    "    ======================\n",
    "    list_word = list of word \n",
    "\n",
    "    usage example \n",
    "    ======================\n",
    "    data_inferential_token = ['untuk', 'recordd', 'mlbb', 'dann', 'pubg', 'kuat', 'kah', 'gan']\n",
    "    data_inferential_token  = steming_word_sastrawi(data_inferential_token)\n",
    "    '''\n",
    "    list_result = []\n",
    "    for word in list_word:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        list_result.append(stemmed_word)\n",
    "    return list_result\n",
    "\n",
    "# apply function steming_word_sastrawi in data_inferential_token\n",
    "data_inferential_token= steming_word_sastrawi(data_inferential_token)\n",
    "data_inferential_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['selamat', 'record', 'mlb', 'pulau', 'pubg', 'kuat', 'gan']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate object that can be used to remove stopword in list word\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_remover = stopword_factory.create_stop_word_remover()\n",
    "\n",
    "def remove_stopword(list_word):\n",
    "    '''\n",
    "    This function is used to remove stopword in list word using Sastrawati \n",
    "    example word \"untuk\" will be removed because it doesn't have meaning\n",
    "\n",
    "    parameter description \n",
    "    ========================\n",
    "    list_word = list of word \n",
    "\n",
    "    example usage \n",
    "    ========================\n",
    "    data_inferential_token = ['untuk', 'recordd', 'mlbb', 'dann', 'pubg', 'kuat', 'kah', 'gan']\n",
    "    data_inferential_token  = remove_stopword(data_inferential_token)ata_inferential_token= remove_stopword(data_inferential_token)\n",
    "    data_inferential_token\n",
    "    '''\n",
    "    list_result = []\n",
    "    for word in list_word:\n",
    "        stopword_word = stopword_remover.remove(word)\n",
    "        if stopword_word != '':\n",
    "            list_result.append(word)\n",
    "    return list_result\n",
    "    \n",
    "data_inferential_token= remove_stopword(data_inferential_token)\n",
    "data_inferential_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[754, 30, 754, 754, 36, 641, 133]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_word_to_nominal_category(list_word):\n",
    "    '''\n",
    "    This function is used to change list word to categorial nominal based on word_dict that created when \n",
    "    process modelling \n",
    "\n",
    "    parameter_description \n",
    "    ========================\n",
    "    list_word = list of word\n",
    "\n",
    "    example usage \n",
    "    ====================\n",
    "    data_inferential_token = ['untuk', 'recordd', 'mlbb', 'dann', 'pubg', 'kuat', 'kah', 'gan']\n",
    "    data_inferential_token  = convert_word_to_nominal_category(data_inferential_token)ata_inferential_token= remove_stopword(data_inferential_token)\n",
    "    data_inferential_token\n",
    "    '''\n",
    "    list_result = []\n",
    "    for word in list_word:\n",
    "        try:\n",
    "            index = word_dict[word]\n",
    "        except:\n",
    "            index = word_dict['ENDPAD']\n",
    "        list_result.append(index)\n",
    "    return list_result\n",
    "\n",
    " \n",
    "data_inferential_token= convert_word_to_nominal_category(data_inferential_token)\n",
    "data_inferential_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[754,  30, 754, 754,  36, 641, 133, 753, 753, 753, 753, 753, 753,\n",
       "        753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753,\n",
       "        753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753,\n",
       "        753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753,\n",
       "        753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753,\n",
       "        753, 753, 753, 753, 753]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 70\n",
    "num_words = len(word_dict)\n",
    "X = [data_inferential_token]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=num_words-1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[754, 30, 754, 754, 36, 641, 133]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_1/Cast:0\", shape=(32,), dtype=float32). Expected shape (None, 70), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32,), dtype=int32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/models/functional.py:244\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_1/Cast:0\", shape=(32,), dtype=float32). Expected shape (None, 70), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32,), dtype=int32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "predict = model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
